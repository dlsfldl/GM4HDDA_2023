{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from loader.MNIST_dataset import MNIST_dataset\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# if you don't have gpu, use cpu as device.\n",
    "device = torch.device('cuda:0')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this exercise, you will implement group equivariant convolutional neural network from the paper \"Group Equivariant Convolutional Networks (2016, T. Cohen et al.)\". \n",
    "* This code is based on https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/DL2/Geometric_deep_learning/tutorial1_regular_group_convolutions.html#2.-Group-Equivariant-Convolutional-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implementing group action\n",
    "\n",
    "First, we implement a simple group action of the cyclic group $c4$. The group $c4$ is consists of rotations by 90 degrees. A matrix parameterization of this group is:\n",
    "\n",
    "$G(r) = \\left[\\begin{array}{cc} \n",
    "\\cos(r\\pi/2) & -\\sin(r\\pi/2)\\\\\n",
    "\\sin(r\\pi/2) & \\cos(r\\pi/2)\n",
    "\\end{array}\\right]$\n",
    "\n",
    "where $0 \\leq r \\leq 3$ and $r \\in \\mathbb{R}$. The group operation is given by the summation of the angle. We assume that the group elements of $c4$ are represented as the integers $r * \\pi / 2$ (i.e., $0, \\pi/2, \\pi, 3\\pi/2$ ), and solve the below programming problems under this assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Write the code for the group operations on $c4$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C4Group(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer('identity', torch.Tensor([0.]))\n",
    "        self.order = torch.tensor(4)\n",
    "\n",
    "    def elements(self):\n",
    "        \"\"\" \n",
    "        out: a tensor containing all group elements in this group.\n",
    "        \"\"\"\n",
    "\n",
    "        out = torch.linspace(\n",
    "            start=0,\n",
    "            end=2 * np.pi * float(self.order - 1) / float(self.order),\n",
    "            steps=self.order,\n",
    "            device=self.identity.device\n",
    "        )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def product(self, h1, h2):\n",
    "        \"\"\" \n",
    "        h1: group element 1 \n",
    "        h2: group element 2\n",
    "        out: group product of two group elements\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############################################\n",
    "        ############### YOUR CODE HERE ###############\n",
    "        ##############################################\n",
    "\n",
    "        return None\n",
    "\n",
    "    def inverse(self, h):\n",
    "        \"\"\" \n",
    "        h: group element\n",
    "        out: group inverse of the group element \n",
    "        \"\"\"\n",
    "\n",
    "        ##############################################\n",
    "        ############### YOUR CODE HERE ###############\n",
    "        ##############################################\n",
    "\n",
    "        return None\n",
    "\n",
    "    def matrix_representation(self, h):\n",
    "        \"\"\" \n",
    "        h: group element\n",
    "        out: matrix representation in R^2 for the group element.\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############################################\n",
    "        ############### YOUR CODE HERE ###############\n",
    "        ##############################################\n",
    "\n",
    "        return None\n",
    "\n",
    "    def left_action_on_R2(self, batch_h, batch_x):\n",
    "        \"\"\"\n",
    "        batch_h: batch of group elements (b)\n",
    "        batch_x: vectors defined in R2   (i, x, y)\n",
    "        out: left action of the elements on a set of vectors in R2 (b, x, y, i)\n",
    "        \"\"\"\n",
    "\n",
    "        ##############################################\n",
    "        ############### YOUR CODE HERE ###############\n",
    "        ##############################################\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def left_action_on_H(self, batch_h, batch_h_prime):\n",
    "        \"\"\" \n",
    "        batch_h: batch of group elements (b)\n",
    "        batch_h_prime: batch of group elements (b)\n",
    "        out: batchwise left group actions (b, b)\n",
    "        \"\"\"\n",
    "        transformed_batch_h = self.product(batch_h.repeat(batch_h_prime.shape[0], 1),\n",
    "                                           batch_h_prime.unsqueeze(-1))\n",
    "        return transformed_batch_h\n",
    "\n",
    "    # this is not the problem\n",
    "    def normalize_group_elements(self, h):\n",
    "        \"\"\" Normalize values of group elements to range between -1 and 1.\n",
    "        The group elements range from 0 to 2pi * (self.order - 1) / self.order,\n",
    "        so we normalize by\n",
    "\n",
    "        @param h: A group element.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        largest_elem = 2 * np.pi * (self.order - 1) / self.order\n",
    "\n",
    "        return (2*h / largest_elem) - 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Your code should pass some example test code.\n",
    "* For this purpose, let's visualize the group action on the image of the arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=320x320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(\"figures/arrow.jpg\")\n",
    "img = img.resize((320, 320))\n",
    "img_tensor = transforms.ToTensor()(img)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* creates a grid of the pixel locations in our image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robotics/anaconda3/envs/GM4HDDA/lib/python3.9/site-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809662/work/aten/src/ATen/native/TensorShape.cpp:2894.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 320, 320])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_grid_R2 = torch.stack(torch.meshgrid(\n",
    "    torch.linspace(-1, 1, img_tensor.shape[-1]),\n",
    "    torch.linspace(-1, 1, img_tensor.shape[-2]),\n",
    "))\n",
    "\n",
    "img_grid_R2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If your code works well, the below code should output the four figures of the rotating arrows something like that:\n",
    "\n",
    "![rotating_arrows](figures/rotating_arrows.PNG)  \n",
    "\n",
    "* Run the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grid_sampler(): argument 'grid' (position 2) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m c4\u001b[39m.\u001b[39melements():\n\u001b[1;32m      4\u001b[0m     transformed_grid \u001b[39m=\u001b[39m c4\u001b[39m.\u001b[39mleft_action_on_R2(g\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), img_grid_R2)\n\u001b[0;32m----> 5\u001b[0m     transformed_img \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mgrid_sample(img_tensor\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m), transformed_grid, align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     transformed_img_list\u001b[39m.\u001b[39mappend(transformed_img)\n\u001b[1;32m      8\u001b[0m x_img \u001b[39m=\u001b[39m make_grid(torch\u001b[39m.\u001b[39mcat(transformed_img_list), nrow\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, value_range\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m), pad_value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/GM4HDDA/lib/python3.9/site-packages/torch/nn/functional.py:4223\u001b[0m, in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4215\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   4216\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4217\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4218\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39malign_corners=True if the old behavior is desired. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4219\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the documentation of grid_sample for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4220\u001b[0m     )\n\u001b[1;32m   4221\u001b[0m     align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 4223\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgrid_sampler(\u001b[39minput\u001b[39;49m, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "\u001b[0;31mTypeError\u001b[0m: grid_sampler(): argument 'grid' (position 2) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "c4 = C4Group()\n",
    "transformed_img_list = []\n",
    "for g in c4.elements():\n",
    "    transformed_grid = c4.left_action_on_R2(g.unsqueeze(0), img_grid_R2)\n",
    "    transformed_img = torch.nn.functional.grid_sample(img_tensor.unsqueeze(0), transformed_grid, align_corners=True)\n",
    "    transformed_img_list.append(transformed_img)\n",
    "\n",
    "x_img = make_grid(torch.cat(transformed_img_list), nrow=4, value_range=(0, 1), pad_value=1)\n",
    "plt.imshow(x_img.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implementing convolutions\n",
    "\n",
    "Then, we implement a group equivariant convolution networks w.r.t. cyclic group $c4$. As shown in the paper, we have to implement two kinds of convolution neural network and one pooling as below:\n",
    "\n",
    "<img src=\"figures/gcnn.PNG\" alt=\"gcnn\" width=\"1000\"/>\n",
    "\n",
    "<!-- ![gcnn](figures/gcnn.PNG)  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1. Z2-P4 convolution\n",
    "\n",
    "Z2-P4 convolution is of the part:\n",
    "\n",
    "<img src=\"figures/gcnn_z2p4.PNG\" alt=\"gcnn_z2p4\" width=\"500\"/>\n",
    "\n",
    "For Z2-P4 convolution network, the filter bank (kernel) class is given as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Z2P4Kernel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.group = C4Group()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # create spatial kernel grid. These are the coordinates on which our kernel weights are defined.\n",
    "        self.register_buffer(\"grid_R2\", torch.stack(torch.meshgrid(\n",
    "            torch.linspace(-1, 1, self.kernel_size),\n",
    "            torch.linspace(-1, 1, self.kernel_size),\n",
    "        )).to(self.group.identity.device))\n",
    "\n",
    "        # transform the grid by the elements in this group.\n",
    "        self.register_buffer(\"transformed_grid_R2\", self.create_transformed_grid_R2())\n",
    "\n",
    "        # create and initialize a set of weights\n",
    "        self.weight = torch.nn.Parameter(torch.zeros((\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        ), device=self.group.identity.device))\n",
    "\n",
    "        # Initialize weights using kaiming uniform intialisation\n",
    "        torch.nn.init.kaiming_uniform_(self.weight.data, a=np.sqrt(5))\n",
    "\n",
    "    def create_transformed_grid_R2(self):\n",
    "        \"\"\"\n",
    "        Transform the created grid by the group action of each group element.\n",
    "        This yields a grid (over H) of spatial grids (over R2). In other words,\n",
    "        a list of grids, each index of which is the original spatial grid transformed by\n",
    "        a corresponding group element in H.\n",
    "        \"\"\"\n",
    "        # Obtain all group elements.\n",
    "        group_elements = self.group.elements()\n",
    "\n",
    "        # Transform the grid defined over R2 with the sampled group elements.\n",
    "        transformed_grid = self.group.left_action_on_R2(\n",
    "            self.group.inverse(group_elements),\n",
    "            self.grid_R2\n",
    "        )\n",
    "        return transformed_grid\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\" \n",
    "        Sample convolution kernels for a given number of group elements\n",
    "        out: filter bank extending over all input channels,\n",
    "            containing kernels transformed for all output group elements.\n",
    "        \"\"\"\n",
    "        # We fold the output channel dim into the input channel dim; this allows\n",
    "        # us to use the torch grid_sample function.\n",
    "        weight = self.weight.view(\n",
    "            1,\n",
    "            self.out_channels * self.in_channels,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        )\n",
    "\n",
    "        # We want a transformed set of weights for each group element so\n",
    "        # we repeat the set of spatial weights along the output group axis\n",
    "        weight = weight.repeat(self.group.elements().numel(), 1, 1, 1)\n",
    "\n",
    "        # Sample the transformed kernels\n",
    "        transformed_weight = torch.nn.functional.grid_sample(\n",
    "            weight,\n",
    "            self.transformed_grid_R2,\n",
    "            mode='bilinear',\n",
    "            padding_mode='zeros',\n",
    "            align_corners=True\n",
    "        )\n",
    "\n",
    "        # Separate input and output channels\n",
    "        transformed_weight = transformed_weight.view(\n",
    "            self.group.elements().numel(),\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        )\n",
    "\n",
    "        # Put the output channel dimension before the output group dimension.\n",
    "        transformed_weight = transformed_weight.transpose(0, 1)\n",
    "\n",
    "        return transformed_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b-1) Below is the figure of the example filter bank. What characteristics does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z2p4kernel_1 = Z2P4Kernel(2, 1, kernel_size=3)\n",
    "weights = z2p4kernel_1.sample()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "\n",
    "# pick an output channel to visualize\n",
    "out_channel_idx = 0\n",
    "\n",
    "# create [in_channels, group_elements] figures\n",
    "fig, ax = plt.subplots(weights.shape[2], weights.shape[1])\n",
    "\n",
    "for in_channel in range(weights.shape[2]):\n",
    "    for group_elem in range(weights.shape[1]):\n",
    "        ax[in_channel, group_elem].imshow(\n",
    "            weights[out_channel_idx, group_elem, in_channel, :, :].detach().numpy()\n",
    "        )\n",
    "\n",
    "fig.text(0.5, 0.04, 'Group elements', ha='center')\n",
    "fig.text(0.04, 0.5, 'Input channels', va='center', rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b-2) Using the kernel code (i.e., Z2P4Kernel), write the code for Z2-P4 convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Z2P4Conv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.kernel = Z2P4Kernel(in_channels, out_channels, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Perform Z2-P4 convolution\n",
    "        x: [batch_dim, in_channels, spatial_dim_1, spatial_dim_2]\n",
    "        out: [batch_dim, out_channels, num_group_elements, spatial_dim_1, spatial_dim_2]\n",
    "        \"\"\"\n",
    "\n",
    "        # obtain convolution kernels transformed under the group\n",
    "        conv_kernels = self.kernel.sample()\n",
    "\n",
    "        ##############################################\n",
    "        ############### YOUR CODE HERE ###############\n",
    "        ##############################################\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2. P4-P4 Convolution\n",
    "\n",
    "P4-P4 convolution is of the part:\n",
    "\n",
    "<img src=\"figures/gcnn_p4p4.PNG\" alt=\"gcnn_p4p4\" width=\"500\"/>\n",
    "\n",
    "For P4-P4 convolution network, the filter bank (kernel) class is given as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P4P4Kernel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.group = C4Group()\n",
    "\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Create a spatial kernel grid\n",
    "        self.register_buffer(\"grid_R2\", torch.stack(torch.meshgrid(\n",
    "            torch.linspace(-1, 1, self.kernel_size),\n",
    "            torch.linspace(-1, 1, self.kernel_size),\n",
    "        )).to(self.group.identity.device))\n",
    "\n",
    "        # The kernel grid now also extends over the group H, as our input\n",
    "        # feature maps contain an additional group dimension\n",
    "        self.register_buffer(\"grid_H\", self.group.elements())\n",
    "        self.register_buffer(\"transformed_grid_R2xH\", self.create_transformed_grid_R2xH())\n",
    "\n",
    "        # create and initialise a set of weights, we will interpolate these\n",
    "        # to create our transformed spatial kernels. Note that our weight\n",
    "        # now also extends over the group H\n",
    "        self.weight = torch.nn.Parameter(torch.zeros((\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.group.elements().numel(), # this is different from the lifting convolution\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        ), device=self.group.identity.device))\n",
    "\n",
    "        # initialize weights using kaiming uniform intialisation\n",
    "        torch.nn.init.kaiming_uniform_(self.weight.data, a=np.sqrt(5))\n",
    "\n",
    "    def create_transformed_grid_R2xH(self):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        # Sample the group\n",
    "        group_elements = self.group.elements()\n",
    "\n",
    "        # Transform the grid defined over R2 with the sampled group elements\n",
    "        transformed_grid_R2 = self.group.left_action_on_R2(\n",
    "            self.group.inverse(group_elements),\n",
    "            self.grid_R2\n",
    "        )\n",
    "\n",
    "        # Transform the grid defined over H with the sampled group elements\n",
    "        transformed_grid_H = self.group.left_action_on_H(\n",
    "            self.group.inverse(group_elements), self.grid_H\n",
    "        )\n",
    "\n",
    "        # Rescale values to between -1 and 1, we do this to please the torch grid_sample\n",
    "        # function.\n",
    "        transformed_grid_H = self.group.normalize_group_elements(transformed_grid_H)\n",
    "\n",
    "        # Create a combined grid as the product of the grids over R2 and H\n",
    "        # repeat R2 along the group dimension, and repeat H along the spatial dimension\n",
    "        # to create a [output_group_elem, num_group_elements, kernel_size, kernel_size, 3] grid\n",
    "        transformed_grid = torch.cat(\n",
    "            (\n",
    "                transformed_grid_R2.view(\n",
    "                    group_elements.numel(),\n",
    "                    1,\n",
    "                    self.kernel_size,\n",
    "                    self.kernel_size,\n",
    "                    2,\n",
    "                ).repeat(1, group_elements.numel(), 1, 1, 1),\n",
    "                transformed_grid_H.view(\n",
    "                    group_elements.numel(),\n",
    "                    group_elements.numel(),\n",
    "                    1,\n",
    "                    1,\n",
    "                    1,\n",
    "                ).repeat(1, 1, self.kernel_size, self.kernel_size, 1, )\n",
    "            ),\n",
    "            dim=-1\n",
    "        )\n",
    "        return transformed_grid\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\" Sample convolution kernels for a given number of group elements\n",
    "\n",
    "        should return:\n",
    "        :return kernels: filter bank extending over all input channels,\n",
    "            containing kernels transformed for all output group elements.\n",
    "        \"\"\"\n",
    "\n",
    "        # fold the output channel dim into the input channel dim; this allows\n",
    "        # us to use the torch grid_sample function\n",
    "        weight = self.weight.view(\n",
    "            1,\n",
    "            self.out_channels * self.in_channels,\n",
    "            self.group.elements().numel(),\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        )\n",
    "\n",
    "        # we want a transformed set of weights for each group element so\n",
    "        weight = weight.repeat(self.group.elements().numel(), 1, 1, 1, 1)\n",
    "\n",
    "        # sample the transformed kernels,\n",
    "        transformed_weight = torch.nn.functional.grid_sample(\n",
    "            weight,\n",
    "            self.transformed_grid_R2xH,\n",
    "            mode='bilinear',\n",
    "            padding_mode='zeros',\n",
    "            align_corners=True\n",
    "        )\n",
    "\n",
    "        # Separate input and output channels. Note we now have a notion of\n",
    "        # input and output group dimensions in our weight matrix!\n",
    "        transformed_weight = transformed_weight.view(\n",
    "            self.group.elements().numel(), # Output group elements (like in the lifting convolutoin)\n",
    "            self.out_channels,\n",
    "            self.in_channels,\n",
    "            self.group.elements().numel(), # Input group elements (due to the additional dimension of our feature map)\n",
    "            self.kernel_size,\n",
    "            self.kernel_size\n",
    "        )\n",
    "\n",
    "        # Put the output channel dimension before the output group dimension.\n",
    "        transformed_weight = transformed_weight.transpose(0, 1)\n",
    "\n",
    "        return transformed_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c-1) Below is the figure of the example filter bank. What characteristics does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p4p4kernel_1 = P4P4Kernel(2, 8, kernel_size=3)\n",
    "weights = p4p4kernel_1.sample()\n",
    "# weights.shape\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "# For ease of viewing, we fold the input group dimension into the spatial x dimension\n",
    "weights_t = weights.view(\n",
    "    weights.shape[0],\n",
    "    weights.shape[1],\n",
    "    weights.shape[2],\n",
    "    weights.shape[3] * weights.shape[4],\n",
    "    weights.shape[5]\n",
    ")\n",
    "\n",
    "# pick an output channel to visualize\n",
    "out_channel_idx = 0\n",
    "\n",
    "# create [in_channels, group_elements] figures\n",
    "fig, ax = plt.subplots(weights.shape[2], weights.shape[1])\n",
    "\n",
    "for in_channel in range(weights.shape[2]):\n",
    "    for group_elem in range(weights.shape[1]):\n",
    "        ax[in_channel, group_elem].imshow(\n",
    "            weights_t[out_channel_idx, group_elem, in_channel, :, :].detach()\n",
    "        )\n",
    "\n",
    "        # Outline the spatial kernel corresponding to the first group element under canonical transformation\n",
    "        rect = matplotlib.patches.Rectangle(\n",
    "            (-0.5, group_elem * weights_t.shape[-1] - 0.5), weights_t.shape[-1], weights_t.shape[-1], linewidth=5, edgecolor='r', facecolor='none')\n",
    "        ax[in_channel, group_elem].add_patch(rect)\n",
    "\n",
    "fig.text(0.5, 0.04, 'Group elements', ha='center')\n",
    "fig.text(0.04, 0.5, 'Input channels / input group elements', va='center', rotation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c-2) Using the kernel code (i.e., P4P4Kernel), write the code for P4-P4 convolution neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P4P4Conv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.kernel = P4P4Kernel(in_channels, out_channels, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Perform P4-P4 convolution\n",
    "        x: [batch_dim, in_channels, num_group_elements, spatial_dim_1, spatial_dim_2]\n",
    "        out: [batch_dim, out_channels, num_group_elements, spatial_dim_1, spatial_dim_2]\n",
    "        \"\"\"\n",
    "\n",
    "        # We obtain convolution kernels transformed under the group\n",
    "        conv_kernels = self.kernel.sample()\n",
    "\n",
    "        ##############################################\n",
    "        ############### YOUR CODE HERE ###############\n",
    "        ##############################################\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-3. P4-Z2 Pooling\n",
    "\n",
    "P4-Z2 Pooling is of the part:\n",
    "\n",
    "<img src=\"figures/gcnn_p4z2.PNG\" alt=\"gcnn_p4z2\" width=\"500\"/>\n",
    "\n",
    "P4-Z2 Pooling is given as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class P4Z2Pooling(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.mean(x, dim=(-3, -2, -1))\n",
    "        x = x.squeeze()\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-4. Using the developed Z2P4Conv, P4P4Conv, and P4Z2Pooling, we can design an example \"group equivariant convolutional neural network\" as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupEquivariantCNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Z2P4Conv(1, 10, kernel_size=3)\n",
    "        self.conv2 = P4P4Conv(10, 10, kernel_size=3)\n",
    "        self.conv3 = P4P4Conv(10, 20, kernel_size=3)\n",
    "        self.conv4 = P4P4Conv(20, 20, kernel_size=3)\n",
    "        self.pooling = P4Z2Pooling()\n",
    "        self.fc = torch.nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pooling(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_last_channels(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(8, 1, 64, 64)\n",
    "eq_net = GroupEquivariantCNN()\n",
    "y_test = eq_net(x_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For comparison, we add the original CNN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=3)\n",
    "        self.conv2 = torch.nn.Conv2d(10, 10, kernel_size=3)\n",
    "        self.conv3 = torch.nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv4 = torch.nn.Conv2d(20, 20, kernel_size=3)\n",
    "        self.fc = torch.nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = torch.mean(x, dim=(-2, -1)) # avg pooling\n",
    "        x = self.fc(x)\n",
    "        x = F.log_softmax(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_last_channels(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.rand(8, 1, 64, 64)\n",
    "net = CNN()\n",
    "y_test = net(x_test)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Check equivariance of the designed networks\n",
    "\n",
    "* load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MNIST_dataset(root='datasets', split='training', digits=[0, 1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = train_ds.data[:10]\n",
    "x_img = make_grid(test_x, nrow=10, value_range=(0, 1), pad_value=1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(x_img.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an image from the test dataset.\n",
    "test_x = train_ds.data[32]\n",
    "test_img = make_grid(test_x, nrow=1, value_range=(0, 1), pad_value=1)\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(test_img.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation angles\n",
    "rots = torch.linspace(0, 360 - 360/4, 4)\n",
    "\n",
    "# rotate the input image\n",
    "rot_digit = torch.stack(tuple(torchvision.transforms.functional.rotate(test_x, a.item(), torchvision.transforms.functional.InterpolationMode.BILINEAR) for a in rots))\n",
    "rot_img = make_grid(rot_digit, nrow=8, value_range=(0, 1), pad_value=1)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(rot_img.permute(1,2,0))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_last_channel = torch.mean(\n",
    "    eq_net.get_last_channels(rot_digit),\n",
    "    dim=-3\n",
    "    )[:,0,...] #if figure looks wierd, try another channel index 0 -> ?\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "fig, ax = plt.subplots(3, 4)\n",
    "for idx in range(4):\n",
    "    ax[0, idx].imshow(\n",
    "        rot_digit[idx, 0, :, :].detach().numpy()\n",
    "    )\n",
    "    ax[1, idx].imshow(\n",
    "        eq_last_channel[idx, :, :].detach().numpy()\n",
    "    )\n",
    "    ax[2, idx].imshow(\n",
    "        torch.rot90(eq_last_channel[idx, :, :], k=-idx).detach().numpy()\n",
    "    )\n",
    "    \n",
    "fig.text(0.04, 0.77, 'input image', va='center', rotation='vertical')\n",
    "fig.text(0.04, 0.51, 'last channel', va='center', rotation='vertical')\n",
    "fig.text(0.04, 0.22, 'aligned last channel', va='center', rotation='vertical')\n",
    "plt.show()\n",
    "print(eq_net(rot_digit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_channel = net.get_last_channels(rot_digit)[:,0,...] #if figure looks wierd, try another channel index 0 -> ?\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "fig, ax = plt.subplots(3, 4)\n",
    "for idx in range(4):\n",
    "    ax[0, idx].imshow(\n",
    "        rot_digit[idx, 0, :, :].detach().numpy()\n",
    "    )\n",
    "    ax[1, idx].imshow(\n",
    "        last_channel[idx, :, :].detach().numpy()\n",
    "    )\n",
    "    ax[2, idx].imshow(\n",
    "        torch.rot90(last_channel[idx, :, :], k=-idx).detach().numpy()\n",
    "    )\n",
    "    \n",
    "fig.text(0.04, 0.77, 'input image', va='center', rotation='vertical')\n",
    "fig.text(0.04, 0.51, 'last channel', va='center', rotation='vertical')\n",
    "fig.text(0.04, 0.22, 'aligned last channel', va='center', rotation='vertical')\n",
    "plt.show()\n",
    "print(net(rot_digit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Does CNN equivariant along translation or rotation? How about group-equivariant CNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compare classification performance of the models.\n",
    "\n",
    "You may use the pretrained model, or train one your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotated dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = MNIST_dataset(root='datasets', split='training', digits=[0, 1, 2, 3, 4, 6])\n",
    "num_data_original_training = 30436  # 10_000\n",
    "num_data_original_test = 6088  # 1_000\n",
    "batch_size = 3000\n",
    "\n",
    "# rotation angles\n",
    "rots = torch.linspace(0, 360 - 360/4, 4)\n",
    "# rotate the dataset image\n",
    "rot_data_stack = torch.stack(\n",
    "    tuple(\n",
    "        torchvision.transforms.functional.rotate(\n",
    "            train_ds.data[:num_data_original_training], a.item(), \n",
    "            torchvision.transforms.functional.InterpolationMode.BILINEAR) \n",
    "        for a in rots)\n",
    "    )\n",
    "rot_data = rot_data_stack.reshape(4*num_data_original_training, *rot_data_stack.shape[2:])\n",
    "rot_target = train_ds.targets[:num_data_original_training].repeat(4)\n",
    "\n",
    "train_ds.data = rot_data\n",
    "train_ds.targets = rot_target\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size)\n",
    "\n",
    "\n",
    "test_ds = MNIST_dataset(root='datasets', split='test', digits=[0, 1, 2, 3, 4, 6])\n",
    "rot_data_stack_test = torch.stack(\n",
    "    tuple(\n",
    "        torchvision.transforms.functional.rotate(\n",
    "            test_ds.data[:num_data_original_test], a.item(), \n",
    "            torchvision.transforms.functional.InterpolationMode.BILINEAR) \n",
    "        for a in rots)\n",
    "    )\n",
    "rot_data_test = rot_data_stack_test.reshape(4*num_data_original_test, *rot_data_stack_test.shape[2:])\n",
    "rot_target_test = test_ds.targets[:num_data_original_test].repeat(4)\n",
    "\n",
    "test_ds.data = rot_data_test\n",
    "test_ds.targets = rot_target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training CNN\n",
    "\n",
    "\n",
    "# net = net.to(device)\n",
    "# max_iter = 100\n",
    "# lr = 0.0001\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# optimizer = torch.optim.Adam(params=net.parameters(), lr=lr)\n",
    "# for i in (range(max_iter)):\n",
    "#     loss_epoch = 0\n",
    "#     count = 0\n",
    "#     for img, label in train_loader:\n",
    "#         output = net(img.to(device))\n",
    "#         loss = loss_fn(output, label.to(device))\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         loss_epoch += loss.item()\n",
    "#         count += 1\n",
    "#     print(f'i = {i}, loss = {loss_epoch/count:3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./net_cpu.pth')\n",
    "net.load_state_dict(state_dict)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing CNN\n",
    "split_size = 500\n",
    "total_pridiction = []\n",
    "for img in test_ds.data.split(split_size=split_size):\n",
    "    output = net(img.to(device))\n",
    "    total_pridiction.append(torch.argmax(output, dim=1))\n",
    "# test_output = net(test_ds.data.to(device))\n",
    "# class_perdiction_test = torch.argmax(test_output, dim=1)\n",
    "total_pridiction = torch.cat(total_pridiction, dim=0)\n",
    "num_data = len(test_ds.data)\n",
    "\n",
    "accuracy = (test_ds.targets.to(device) == total_pridiction).sum()/num_data * 100\n",
    "\n",
    "print(f'Final accuracy = {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training equivariant CNN\n",
    "\n",
    "\n",
    "\n",
    "# eq_net = eq_net.to(device)\n",
    "# max_iter = 20\n",
    "# lr = 0.0001\n",
    "# loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer = torch.optim.Adam(params=eq_net.parameters(), lr=lr)\n",
    "# for i in (range(max_iter)):\n",
    "#     loss_epoch = 0\n",
    "#     count = 0\n",
    "#     for img, label in train_loader:\n",
    "#         output = eq_net(img.to(device))\n",
    "#         loss = loss_fn(output, label.to(device))\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         loss_epoch += loss.item()\n",
    "#         count += 1\n",
    "#         print(f'i = {i}, loss = {loss_epoch/count:3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('./eqnet_cpu.pth')\n",
    "eq_net.load_state_dict(state_dict)\n",
    "eq_net = eq_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing CNN\n",
    "split_size = 500\n",
    "total_pridiction = []\n",
    "for img in test_ds.data.split(split_size=split_size):\n",
    "    output = eq_net(img.to(device))\n",
    "    total_pridiction.append(torch.argmax(output, dim=1))\n",
    "# test_output = net(test_ds.data.to(device))\n",
    "# class_perdiction_test = torch.argmax(test_output, dim=1)\n",
    "total_pridiction = torch.cat(total_pridiction, dim=0)\n",
    "num_data = len(test_ds.data)\n",
    "\n",
    "accuracy = (test_ds.targets.to(device) == total_pridiction).sum()/num_data * 100\n",
    "\n",
    "print(f'Final accuracy = {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Why is the classification performance of the equivariant model better?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('GM4HDDA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fb749bfe1691104d2411e1fc5a43a8911c8176e71d1b0a5a4fd2193586e7e04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
